```
requirements: none
zig build and then you are done
ai slop is zen
```

# flux2.zig ğŸ¦âš¡

> **Origin prompt:**
> *"https://github.com/antirez/flux2.c analyze those code base and plan a conversion to zig for the main programs and libraries except the blas stuff which seems like a much bigger project. This was coded by Claude in a weekend with skilled human guidance but I want a zig version zig build zig code"*

A pure Zig port of [antirez/flux2.c](https://github.com/antirez/flux2.c) â€” FLUX.2-klein-4B image generation inference. ğŸ–¼ï¸âœ¨

No Python. ğŸâŒ No PyTorch. ğŸ”¥âŒ No CUDA toolkit. ğŸ®âŒ Just `zig build`. âš¡ğŸ’¯

## Status ğŸ“Š

ğŸš§ **Work in Progress** â€” Foundation complete, inference pipeline in development.

| Component | Status | Notes |
|-----------|--------|-------|
| `tensor.zig` | âœ… | Multi-dim arrays, views, reshape |
| `kernels.zig` | âœ… | SIMD matmul, GELU, softmax, RMSNorm, RoPE |
| `safetensors.zig` | âœ… | Model loading, mmap, BF16â†’F32 |
| `image.zig` | âœ… | PPM I/O, tensor conversion |
| `server.zig` | âœ… | REPL mode for memory-resident ops |
| `hfd` tool | âœ… | HuggingFace downloader (replaces Python ğŸğŸ’€) |
| Transformer | ğŸ”¨ | In progress |
| VAE | ğŸ”¨ | In progress |
| Qwen3 encoder | ğŸ”¨ | In progress |
| Tokenizer | ğŸ”¨ | In progress |

## Quick Start ğŸš€

```bash
# Build everything ğŸ”§
zig build

# Download the model (~22GB) - no Python needed! ğŸ“¥ğŸ‰
./zig-out/bin/hfd black-forest-labs/FLUX.2-klein-4B -o flux-klein-model

# Generate an image (once inference is complete) ğŸ¨
./zig-out/bin/flux -d flux-klein-model -p "A fluffy cat" -o cat.ppm
```

## Building ğŸ—ï¸

```bash
zig build              # Build flux CLI + hfd ğŸ”¨
zig build server       # Run flux-server (memory-resident) ğŸ–¥ï¸
zig build hfd          # Run HuggingFace downloader ğŸ“¦
zig build test         # Run tests ğŸ§ª
zig build bench        # Run kernel benchmarks ğŸ“ˆ
```

Requires Zig 0.15.2+ ğŸ¦

## Project Structure ğŸ“

```
flux2.zig/
â”œâ”€â”€ build.zig
â””â”€â”€ src/
    â”œâ”€â”€ flux.zig          # Public API ğŸ”Œ
    â”œâ”€â”€ main.zig          # CLI: flux ğŸ’»
    â”œâ”€â”€ server.zig        # CLI: flux-server (REPL/HTTP) ğŸŒ
    â”œâ”€â”€ hfd.zig           # CLI: hfd (HuggingFace downloader) ğŸ“¥
    â”œâ”€â”€ tensor.zig        # N-dimensional tensors ğŸ§®
    â”œâ”€â”€ kernels.zig       # SIMD compute primitives âš¡
    â”œâ”€â”€ safetensors.zig   # HuggingFace model format ğŸ¤—
    â”œâ”€â”€ image.zig         # Image I/O ğŸ–¼ï¸
    â””â”€â”€ bench.zig         # Benchmarks ğŸï¸
```

## Tools ğŸ› ï¸

### `hfd` â€” HuggingFace Downloader ğŸ“¥

A pure Zig replacement for `pip install huggingface_hub && python download_model.py`: ğŸâ¡ï¸ğŸ¦

```bash
# Download a model ğŸ“¦
./zig-out/bin/hfd black-forest-labs/FLUX.2-klein-4B

# Download only safetensors ğŸ¯
./zig-out/bin/hfd black-forest-labs/FLUX.2-klein-4B --include "*.safetensors"

# Resume interrupted download â¸ï¸â–¶ï¸
./zig-out/bin/hfd black-forest-labs/FLUX.2-klein-4B --resume

# Dry run - see what would be downloaded ğŸ‘€
./zig-out/bin/hfd black-forest-labs/FLUX.2-klein-4B --dry-run

# Gated models (requires token) ğŸ”
export HF_TOKEN=hf_xxxxx
./zig-out/bin/hfd meta-llama/Llama-2-7b
```

Features: âœ¨
- ğŸ”’ SHA256 verification for LFS files
- â¸ï¸ Resume interrupted downloads
- ğŸ¯ Include/exclude glob patterns
- ğŸ“Š Progress bar with transfer stats

### `flux-server` â€” Memory-Resident Generation ğŸ§ ğŸ’¾

Keep the model loaded for fast repeated generations:

```bash
./zig-out/bin/flux-server -d flux-klein-model --repl

> {"prompt": "A cat", "output": "cat.ppm"}
Generated: cat.ppm (256x256) in 2100ms ğŸ±
> {"prompt": "A dog", "output": "dog.ppm"}
Generated: dog.ppm (256x256) in 1950ms ğŸ•
> quit
```

## Why Zig? ğŸ¦ğŸ’ª

- ğŸš« **Zero dependencies** â€” No libc required, static binaries
- âš¡ **Portable SIMD** â€” `@Vector` works on x86 AVX, ARM NEON, WASM
- ğŸ§  **Explicit allocators** â€” No hidden allocations, predictable memory
- ğŸ”® **Comptime** â€” Shape validation at compile time
- ğŸ”— **C interop** â€” Can still link OpenBLAS/Accelerate if needed

## Performance ğŸï¸ğŸ’¨

Pure Zig SIMD kernels (no BLAS):

```
Matrix Multiplication (C = A @ B): ğŸ§®
  64x64x64:    0.15 ms
  256x256x256: 8.2 ms
  512x512x512: 65 ms
  1024x1024x1024: 520 ms

GELU Activation: âš¡
  n=65536: 45 Âµs, 1450 M elem/s

RMS Normalization: ğŸ“
  hidden_size=3072: 1.2 Âµs
```

~10-30x slower than optimized BLAS. Acceptable for CPU inference, room for optimization. ğŸ“ˆ

## Roadmap ğŸ—ºï¸

1. **Phase 1** âœ… Foundation â€” tensor, kernels, safetensors, image
2. **Phase 2** ğŸ”¨ Neural network layers â€” transformer, VAE, Qwen3
3. **Phase 3** â³ Integration â€” tokenizer, sampler, end-to-end
4. **Phase 4** â³ Optimization â€” better tiling, cache blocking, optional BLAS

## Acknowledgments ğŸ™

- ğŸ© [antirez/flux2.c](https://github.com/antirez/flux2.c) â€” The original C implementation (MIT)
- ğŸŒ² [Black Forest Labs](https://blackforestlabs.ai/) â€” FLUX.2-klein model (Apache 2.0)
- ğŸ¤– Claude â€” Code generation for both the original C and this Zig port

## License ğŸ“„

MIT â€” Same as the original flux2.c

---

*"I believe that inference systems not using the Python stack are a way to free open models usage and make AI more accessible."* â€” antirez ğŸ§™â€â™‚ï¸

---

ğŸ¤– *This port was vibe-coded by Claude (Opus 4.5) with human guidance. The emojis are a feature, not a bug.* ğŸ¦âœ¨ğŸš€
